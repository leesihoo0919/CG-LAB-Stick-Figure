{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80d2bf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13997, 1024)\n",
      "(13997, 22)\n",
      "(4666, 1024)\n",
      "(4666, 22)\n",
      "(4666, 1024)\n",
      "(4666, 22)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.metrics import MeanSquaredError\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 이미지 데이터셋과 CSV 파일 경로\n",
    "image_folder = \"./sf/\"  # 이미지 폴더 경로\n",
    "csv_file = \"./joint_all.csv\"  # CSV 파일 경로\n",
    "\n",
    "# 이미지 크기\n",
    "image_size = (32, 32)\n",
    "\n",
    "# 이미지 데이터(특징값) 불러오는 함수\n",
    "def load_images(image_folder, image_size):\n",
    "    images = []\n",
    "    for filename in os.listdir(image_folder):\n",
    "        img = cv2.imread(os.path.join(image_folder, filename), cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            _, binary_image = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "            binary_image = binary_image.astype(np.uint8)\n",
    "            distance_transform = cv2.distanceTransform(binary_image, cv2.DIST_L2, 5)\n",
    "            img = cv2.resize(distance_transform, image_size)\n",
    "            img = img.flatten()\n",
    "            images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "# CSV 파일 불러오는 함수\n",
    "def load_csv(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    return df.values[:, 1:]  # 첫 번째 열(이미지 이름) 제외\n",
    "\n",
    "# 이미지 데이터셋과 CSV 파일 로드\n",
    "image_data = load_images(image_folder, image_size)\n",
    "csv_data = load_csv(csv_file)\n",
    "\n",
    "for i in range(len(csv_data)):\n",
    "    if csv_data[i][6] > csv_data[i][10]:\n",
    "        for j in range(6,10):\n",
    "            tmp = csv_data[i][j]\n",
    "            csv_data[i][j] = csv_data[i][j+4]\n",
    "            csv_data[i][j+4] = tmp\n",
    "\n",
    "    if csv_data[i][14] > csv_data[i][18]:\n",
    "        for j in range(14,18):\n",
    "            tmp = csv_data[i][j]\n",
    "            csv_data[i][j] = csv_data[i][j+4]\n",
    "            csv_data[i][j+4] = tmp\n",
    "\n",
    "# 데이터셋 셔플링\n",
    "indices = np.arange(len(csv_data))\n",
    "np.random.shuffle(indices)\n",
    "shuffled_images = image_data[indices]\n",
    "shuffled_csv = csv_data[indices]\n",
    "\n",
    "\n",
    "# 데이터셋 분할 (train:validation:test = 0.6:0.2:0.2)\n",
    "train_images, test_images, train_csv, test_csv = train_test_split(shuffled_images, shuffled_csv, test_size=0.2, random_state=42)\n",
    "train_images, val_images, train_csv, val_csv = train_test_split(train_images, train_csv, test_size=0.25, random_state=42)\n",
    "\n",
    "# 데이터 전처리 (입력 이미지와 목표값 간의 매핑 필요)\n",
    "x_train = train_images.reshape(-1, *image_size, 1).astype('float32') / 255.0\n",
    "y_train = train_csv.astype('float32')\n",
    "\n",
    "x_val = val_images.reshape(-1, *image_size, 1).astype('float32') / 255.0\n",
    "y_val = val_csv.astype('float32')\n",
    "\n",
    "x_test = test_images.reshape(-1, *image_size, 1).astype('float32') / 255.0\n",
    "y_test = test_csv.astype('float32')\n",
    "\n",
    "\n",
    "print(train_images.shape)\n",
    "print(train_csv.shape)\n",
    "print(test_images.shape)\n",
    "print(test_csv.shape)\n",
    "print(val_images.shape)\n",
    "print(val_csv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a876ae58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "438/438 [==============================] - 36s 43ms/step - loss: 2685.7935 - mae: 34.0548 - val_loss: 1684.8361 - val_mae: 31.1380\n",
      "Epoch 2/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1471.9913 - mae: 27.1971 - val_loss: 1543.0602 - val_mae: 28.7110\n",
      "Epoch 3/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1463.6439 - mae: 27.1060 - val_loss: 1520.7063 - val_mae: 28.1749\n",
      "Epoch 4/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1458.0292 - mae: 27.0836 - val_loss: 1651.6920 - val_mae: 30.6253\n",
      "Epoch 5/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1405.3467 - mae: 26.5113 - val_loss: 1519.1779 - val_mae: 29.0662\n",
      "Epoch 6/1000\n",
      "438/438 [==============================] - 17s 38ms/step - loss: 1379.1783 - mae: 26.2566 - val_loss: 1619.1105 - val_mae: 30.5627\n",
      "Epoch 7/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1364.4978 - mae: 26.0546 - val_loss: 1785.2285 - val_mae: 32.9015\n",
      "Epoch 8/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1354.8704 - mae: 25.9463 - val_loss: 1415.3640 - val_mae: 27.3261\n",
      "Epoch 9/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1347.4840 - mae: 25.8395 - val_loss: 1367.5808 - val_mae: 26.4130\n",
      "Epoch 10/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1336.1533 - mae: 25.7034 - val_loss: 1343.0306 - val_mae: 25.7365\n",
      "Epoch 11/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1331.0381 - mae: 25.6465 - val_loss: 1531.3750 - val_mae: 29.2308\n",
      "Epoch 12/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1322.8973 - mae: 25.5493 - val_loss: 1360.0852 - val_mae: 26.5222\n",
      "Epoch 13/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1313.8226 - mae: 25.4019 - val_loss: 1313.5864 - val_mae: 25.2609\n",
      "Epoch 14/1000\n",
      "438/438 [==============================] - 16s 38ms/step - loss: 1304.5627 - mae: 25.2828 - val_loss: 1323.7268 - val_mae: 25.8601\n",
      "Epoch 15/1000\n",
      "438/438 [==============================] - 16s 37ms/step - loss: 1299.1545 - mae: 25.2323 - val_loss: 1333.7916 - val_mae: 26.1326\n",
      "Epoch 16/1000\n",
      "438/438 [==============================] - 17s 38ms/step - loss: 1291.5732 - mae: 25.1028 - val_loss: 1341.2491 - val_mae: 26.2560\n",
      "Epoch 17/1000\n",
      "438/438 [==============================] - 17s 38ms/step - loss: 1285.5378 - mae: 25.0495 - val_loss: 1415.9194 - val_mae: 27.6491\n",
      "Epoch 18/1000\n",
      "438/438 [==============================] - 17s 38ms/step - loss: 1281.7628 - mae: 24.9853 - val_loss: 1301.0902 - val_mae: 25.4069\n",
      "Epoch 19/1000\n",
      "438/438 [==============================] - 17s 38ms/step - loss: 1275.3370 - mae: 24.9148 - val_loss: 1303.8439 - val_mae: 25.6499\n",
      "Epoch 20/1000\n",
      "438/438 [==============================] - 17s 38ms/step - loss: 1272.1448 - mae: 24.9177 - val_loss: 1419.0658 - val_mae: 27.6178\n",
      "Epoch 21/1000\n",
      "438/438 [==============================] - 16s 38ms/step - loss: 1267.4326 - mae: 24.8408 - val_loss: 1343.5121 - val_mae: 26.2539\n",
      "Epoch 22/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1261.1244 - mae: 24.7744 - val_loss: 1382.0862 - val_mae: 27.2055\n",
      "Epoch 23/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1256.6781 - mae: 24.7592 - val_loss: 1348.2722 - val_mae: 26.4561\n",
      "Epoch 24/1000\n",
      "438/438 [==============================] - 17s 38ms/step - loss: 1248.0283 - mae: 24.6546 - val_loss: 1383.2329 - val_mae: 27.1323\n",
      "Epoch 25/1000\n",
      "438/438 [==============================] - 17s 40ms/step - loss: 1243.0350 - mae: 24.6012 - val_loss: 1331.1855 - val_mae: 26.5205\n",
      "Epoch 26/1000\n",
      "438/438 [==============================] - 18s 40ms/step - loss: 1238.6619 - mae: 24.5714 - val_loss: 1308.5370 - val_mae: 25.9498\n",
      "Epoch 27/1000\n",
      "438/438 [==============================] - 17s 38ms/step - loss: 1230.9762 - mae: 24.4677 - val_loss: 1284.5306 - val_mae: 25.3160\n",
      "Epoch 28/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1226.2490 - mae: 24.3679 - val_loss: 1270.8274 - val_mae: 25.0557\n",
      "Epoch 29/1000\n",
      "438/438 [==============================] - 17s 38ms/step - loss: 1221.4585 - mae: 24.3528 - val_loss: 1284.0807 - val_mae: 25.6634\n",
      "Epoch 30/1000\n",
      "438/438 [==============================] - 17s 40ms/step - loss: 1212.0162 - mae: 24.2285 - val_loss: 1242.6974 - val_mae: 24.7044\n",
      "Epoch 31/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1210.6166 - mae: 24.2611 - val_loss: 1286.5260 - val_mae: 25.7353\n",
      "Epoch 32/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1204.7426 - mae: 24.1861 - val_loss: 1251.6960 - val_mae: 25.1822\n",
      "Epoch 33/1000\n",
      "438/438 [==============================] - 17s 40ms/step - loss: 1198.1816 - mae: 24.0844 - val_loss: 1322.5411 - val_mae: 26.4353\n",
      "Epoch 34/1000\n",
      "438/438 [==============================] - 17s 40ms/step - loss: 1193.1727 - mae: 24.0723 - val_loss: 1318.5830 - val_mae: 26.2690\n",
      "Epoch 35/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1191.3876 - mae: 24.0372 - val_loss: 1299.1200 - val_mae: 26.0968\n",
      "Epoch 36/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1179.3103 - mae: 23.8782 - val_loss: 1277.8347 - val_mae: 25.8913\n",
      "Epoch 37/1000\n",
      "438/438 [==============================] - 17s 40ms/step - loss: 1174.1833 - mae: 23.8318 - val_loss: 1294.4587 - val_mae: 26.1363\n",
      "Epoch 38/1000\n",
      "438/438 [==============================] - 18s 40ms/step - loss: 1172.2832 - mae: 23.8365 - val_loss: 1244.5510 - val_mae: 24.7444\n",
      "Epoch 39/1000\n",
      "438/438 [==============================] - 17s 40ms/step - loss: 1165.3073 - mae: 23.7639 - val_loss: 1248.1689 - val_mae: 25.1726\n",
      "Epoch 40/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1159.4442 - mae: 23.6547 - val_loss: 1268.0867 - val_mae: 25.5451\n",
      "Epoch 41/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1154.2072 - mae: 23.6694 - val_loss: 1200.5786 - val_mae: 24.2254\n",
      "Epoch 42/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1147.6019 - mae: 23.5901 - val_loss: 1200.2568 - val_mae: 24.2098\n",
      "Epoch 43/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1141.7881 - mae: 23.5152 - val_loss: 1246.0299 - val_mae: 25.2171\n",
      "Epoch 44/1000\n",
      "438/438 [==============================] - 17s 38ms/step - loss: 1135.9139 - mae: 23.4423 - val_loss: 1214.8213 - val_mae: 24.7344\n",
      "Epoch 45/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1128.5995 - mae: 23.3825 - val_loss: 1211.3423 - val_mae: 24.8838\n",
      "Epoch 46/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1120.2458 - mae: 23.3051 - val_loss: 1238.1311 - val_mae: 25.0831\n",
      "Epoch 47/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1113.9486 - mae: 23.2334 - val_loss: 1189.8480 - val_mae: 24.1794\n",
      "Epoch 48/1000\n",
      "438/438 [==============================] - 16s 37ms/step - loss: 1109.6798 - mae: 23.1762 - val_loss: 1178.0463 - val_mae: 23.9599\n",
      "Epoch 49/1000\n",
      "438/438 [==============================] - 16s 37ms/step - loss: 1105.9252 - mae: 23.1669 - val_loss: 1176.4543 - val_mae: 23.9633\n",
      "Epoch 50/1000\n",
      "438/438 [==============================] - 16s 36ms/step - loss: 1097.0959 - mae: 23.0751 - val_loss: 1213.4761 - val_mae: 24.8374\n",
      "Epoch 51/1000\n",
      "438/438 [==============================] - 16s 37ms/step - loss: 1088.4523 - mae: 22.9601 - val_loss: 1178.0583 - val_mae: 24.0132\n",
      "Epoch 52/1000\n",
      "438/438 [==============================] - 16s 37ms/step - loss: 1076.9773 - mae: 22.8360 - val_loss: 1197.4303 - val_mae: 24.4956\n",
      "Epoch 53/1000\n",
      "438/438 [==============================] - 16s 37ms/step - loss: 1076.8684 - mae: 22.8843 - val_loss: 1172.2478 - val_mae: 24.0769\n",
      "Epoch 54/1000\n",
      "438/438 [==============================] - 16s 37ms/step - loss: 1066.5771 - mae: 22.7463 - val_loss: 1216.5977 - val_mae: 24.9658\n",
      "Epoch 55/1000\n",
      "438/438 [==============================] - 16s 37ms/step - loss: 1063.0092 - mae: 22.7530 - val_loss: 1160.9359 - val_mae: 23.8101\n",
      "Epoch 56/1000\n",
      "438/438 [==============================] - 17s 38ms/step - loss: 1051.1552 - mae: 22.5960 - val_loss: 1262.0165 - val_mae: 25.9388\n",
      "Epoch 57/1000\n",
      "438/438 [==============================] - 16s 37ms/step - loss: 1040.8256 - mae: 22.4888 - val_loss: 1159.1628 - val_mae: 23.8803\n",
      "Epoch 58/1000\n",
      "438/438 [==============================] - 16s 37ms/step - loss: 1041.5139 - mae: 22.5121 - val_loss: 1200.1943 - val_mae: 24.4265\n",
      "Epoch 59/1000\n",
      "438/438 [==============================] - 16s 37ms/step - loss: 1032.4878 - mae: 22.4249 - val_loss: 1188.3333 - val_mae: 24.7558\n",
      "Epoch 60/1000\n",
      "438/438 [==============================] - 16s 37ms/step - loss: 1018.6611 - mae: 22.2527 - val_loss: 1166.1293 - val_mae: 23.9231\n",
      "Epoch 61/1000\n",
      "438/438 [==============================] - 16s 36ms/step - loss: 1013.6357 - mae: 22.2208 - val_loss: 1146.6144 - val_mae: 23.8646\n",
      "Epoch 62/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 1010.4094 - mae: 22.2060 - val_loss: 1183.9093 - val_mae: 24.5595\n",
      "Epoch 63/1000\n",
      "438/438 [==============================] - 17s 40ms/step - loss: 999.2497 - mae: 22.0644 - val_loss: 1200.9659 - val_mae: 24.7569\n",
      "Epoch 64/1000\n",
      "438/438 [==============================] - 17s 38ms/step - loss: 990.9297 - mae: 21.9611 - val_loss: 1135.3202 - val_mae: 23.5498\n",
      "Epoch 65/1000\n",
      "438/438 [==============================] - 16s 37ms/step - loss: 984.9376 - mae: 21.9132 - val_loss: 1172.6477 - val_mae: 24.1796\n",
      "Epoch 66/1000\n",
      "438/438 [==============================] - 18s 41ms/step - loss: 974.5726 - mae: 21.8084 - val_loss: 1179.5250 - val_mae: 24.5621\n",
      "Epoch 67/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 969.1127 - mae: 21.7793 - val_loss: 1203.6238 - val_mae: 25.0595\n",
      "Epoch 68/1000\n",
      "438/438 [==============================] - 17s 39ms/step - loss: 964.1145 - mae: 21.7217 - val_loss: 1106.8164 - val_mae: 22.9665\n",
      "Epoch 69/1000\n",
      "438/438 [==============================] - 17s 40ms/step - loss: 958.5250 - mae: 21.6958 - val_loss: 1156.2710 - val_mae: 23.8982\n",
      "Epoch 70/1000\n",
      "438/438 [==============================] - 18s 42ms/step - loss: 955.4902 - mae: 21.6506 - val_loss: 1173.4790 - val_mae: 24.5675\n",
      "Epoch 71/1000\n",
      "438/438 [==============================] - 15s 34ms/step - loss: 940.2752 - mae: 21.4772 - val_loss: 1159.8010 - val_mae: 24.3574\n",
      "Epoch 72/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 930.5080 - mae: 21.3617 - val_loss: 1103.4451 - val_mae: 22.7974\n",
      "Epoch 73/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 928.6874 - mae: 21.3210 - val_loss: 1188.0061 - val_mae: 24.8402\n",
      "Epoch 74/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 920.9412 - mae: 21.2549 - val_loss: 1113.4202 - val_mae: 23.1799\n",
      "Epoch 75/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 914.2610 - mae: 21.1656 - val_loss: 1118.3536 - val_mae: 23.3974\n",
      "Epoch 76/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 904.6808 - mae: 21.0474 - val_loss: 1107.9222 - val_mae: 23.2677\n",
      "Epoch 77/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 899.3774 - mae: 21.0104 - val_loss: 1102.8105 - val_mae: 23.1116\n",
      "Epoch 78/1000\n",
      "438/438 [==============================] - 12s 28ms/step - loss: 894.4907 - mae: 20.9678 - val_loss: 1147.4431 - val_mae: 24.2532\n",
      "Epoch 79/1000\n",
      "438/438 [==============================] - 12s 28ms/step - loss: 886.2636 - mae: 20.8503 - val_loss: 1109.9969 - val_mae: 23.2561\n",
      "Epoch 80/1000\n",
      "438/438 [==============================] - 12s 28ms/step - loss: 882.3976 - mae: 20.8190 - val_loss: 1141.4987 - val_mae: 23.7004\n",
      "Epoch 81/1000\n",
      "438/438 [==============================] - 12s 28ms/step - loss: 879.6196 - mae: 20.7866 - val_loss: 1158.0886 - val_mae: 24.2695\n",
      "Epoch 82/1000\n",
      "438/438 [==============================] - 12s 28ms/step - loss: 868.7047 - mae: 20.6959 - val_loss: 1180.7106 - val_mae: 24.8487\n",
      "Epoch 83/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 866.7699 - mae: 20.6489 - val_loss: 1090.2577 - val_mae: 23.0690\n",
      "Epoch 84/1000\n",
      "438/438 [==============================] - 12s 28ms/step - loss: 858.0853 - mae: 20.5753 - val_loss: 1082.8705 - val_mae: 22.7774\n",
      "Epoch 85/1000\n",
      "438/438 [==============================] - 12s 28ms/step - loss: 850.3441 - mae: 20.4961 - val_loss: 1094.0076 - val_mae: 22.9657\n",
      "Epoch 86/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 843.7567 - mae: 20.3745 - val_loss: 1083.5419 - val_mae: 22.7505\n",
      "Epoch 87/1000\n",
      "438/438 [==============================] - 12s 28ms/step - loss: 840.5673 - mae: 20.3914 - val_loss: 1129.0167 - val_mae: 24.0792\n",
      "Epoch 88/1000\n",
      "438/438 [==============================] - 12s 28ms/step - loss: 834.3719 - mae: 20.3008 - val_loss: 1073.5581 - val_mae: 22.5125\n",
      "Epoch 89/1000\n",
      "438/438 [==============================] - 12s 28ms/step - loss: 832.0663 - mae: 20.2976 - val_loss: 1081.4431 - val_mae: 22.8588\n",
      "Epoch 90/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 825.5849 - mae: 20.1969 - val_loss: 1055.5349 - val_mae: 22.3483\n",
      "Epoch 91/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 818.2489 - mae: 20.1056 - val_loss: 1078.5820 - val_mae: 22.9848\n",
      "Epoch 92/1000\n",
      "438/438 [==============================] - 12s 28ms/step - loss: 814.5422 - mae: 20.0749 - val_loss: 1071.0031 - val_mae: 22.6033\n",
      "Epoch 93/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 815.8682 - mae: 20.1030 - val_loss: 1056.1063 - val_mae: 22.3721\n",
      "Epoch 94/1000\n",
      "438/438 [==============================] - 12s 29ms/step - loss: 808.6641 - mae: 20.0154 - val_loss: 1066.3610 - val_mae: 22.6659\n",
      "Epoch 95/1000\n",
      "438/438 [==============================] - 12s 29ms/step - loss: 798.7401 - mae: 19.8683 - val_loss: 1057.7699 - val_mae: 22.5212\n",
      "Epoch 96/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 796.2233 - mae: 19.8541 - val_loss: 1067.8969 - val_mae: 22.8002\n",
      "Epoch 97/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 791.6749 - mae: 19.8275 - val_loss: 1060.5956 - val_mae: 22.7337\n",
      "Epoch 98/1000\n",
      "438/438 [==============================] - 12s 28ms/step - loss: 785.6777 - mae: 19.7300 - val_loss: 1061.4403 - val_mae: 22.3760\n",
      "Epoch 99/1000\n",
      "438/438 [==============================] - 12s 28ms/step - loss: 782.5060 - mae: 19.6890 - val_loss: 1045.8373 - val_mae: 22.1976\n",
      "Epoch 100/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 774.1672 - mae: 19.5840 - val_loss: 1047.2258 - val_mae: 22.3560\n",
      "Epoch 101/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 776.6821 - mae: 19.6467 - val_loss: 1033.4307 - val_mae: 22.0846\n",
      "Epoch 102/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 769.6146 - mae: 19.5439 - val_loss: 1083.0000 - val_mae: 22.9452\n",
      "Epoch 103/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 765.4399 - mae: 19.5157 - val_loss: 1025.4498 - val_mae: 21.8659\n",
      "Epoch 104/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 761.4625 - mae: 19.4450 - val_loss: 1037.1538 - val_mae: 22.2905\n",
      "Epoch 105/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 754.9728 - mae: 19.3754 - val_loss: 1072.7312 - val_mae: 23.0185\n",
      "Epoch 106/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 750.1271 - mae: 19.2974 - val_loss: 1038.9817 - val_mae: 22.3307\n",
      "Epoch 107/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 746.9410 - mae: 19.2643 - val_loss: 1032.4203 - val_mae: 22.2333\n",
      "Epoch 108/1000\n",
      "438/438 [==============================] - 15s 35ms/step - loss: 743.4546 - mae: 19.2327 - val_loss: 1044.0680 - val_mae: 22.2011\n",
      "Epoch 109/1000\n",
      "438/438 [==============================] - 15s 34ms/step - loss: 738.0318 - mae: 19.1519 - val_loss: 1051.4503 - val_mae: 22.5265\n",
      "Epoch 110/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 735.7548 - mae: 19.1254 - val_loss: 1045.7780 - val_mae: 22.5217\n",
      "Epoch 111/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 726.5483 - mae: 19.0157 - val_loss: 1026.0576 - val_mae: 22.0960\n",
      "Epoch 112/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 726.3002 - mae: 19.0304 - val_loss: 1056.8060 - val_mae: 22.6483\n",
      "Epoch 113/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 720.0362 - mae: 18.9426 - val_loss: 1013.0262 - val_mae: 21.8112\n",
      "Epoch 114/1000\n",
      "438/438 [==============================] - 12s 28ms/step - loss: 720.7559 - mae: 18.9686 - val_loss: 1036.8230 - val_mae: 21.8948\n",
      "Epoch 115/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 713.9703 - mae: 18.8763 - val_loss: 1053.9609 - val_mae: 22.6486\n",
      "Epoch 116/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 710.4948 - mae: 18.8435 - val_loss: 1013.0329 - val_mae: 21.8583\n",
      "Epoch 117/1000\n",
      "438/438 [==============================] - 12s 28ms/step - loss: 703.2582 - mae: 18.7577 - val_loss: 1029.5304 - val_mae: 22.0488\n",
      "Epoch 118/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 703.4394 - mae: 18.7989 - val_loss: 1024.0006 - val_mae: 22.1158\n",
      "Epoch 119/1000\n",
      "438/438 [==============================] - 15s 33ms/step - loss: 701.8387 - mae: 18.7456 - val_loss: 1031.1593 - val_mae: 22.1124\n",
      "Epoch 120/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 701.3150 - mae: 18.7144 - val_loss: 1043.0854 - val_mae: 22.5415\n",
      "Epoch 121/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 696.6671 - mae: 18.6764 - val_loss: 1009.0986 - val_mae: 21.6475\n",
      "Epoch 122/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 688.6507 - mae: 18.5672 - val_loss: 1043.2783 - val_mae: 22.3268\n",
      "Epoch 123/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 693.8659 - mae: 18.6178 - val_loss: 1007.6064 - val_mae: 21.6650\n",
      "Epoch 124/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 683.0263 - mae: 18.4925 - val_loss: 1019.9734 - val_mae: 21.8757\n",
      "Epoch 125/1000\n",
      "438/438 [==============================] - 12s 28ms/step - loss: 680.6406 - mae: 18.4517 - val_loss: 1010.2445 - val_mae: 21.6802\n",
      "Epoch 126/1000\n",
      "438/438 [==============================] - 12s 28ms/step - loss: 679.9096 - mae: 18.4899 - val_loss: 1011.1459 - val_mae: 21.8307\n",
      "Epoch 127/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 675.7914 - mae: 18.4188 - val_loss: 1052.6943 - val_mae: 22.6707\n",
      "Epoch 128/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 675.0767 - mae: 18.4010 - val_loss: 1005.0959 - val_mae: 21.7953\n",
      "Epoch 129/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 670.0577 - mae: 18.3475 - val_loss: 1019.1827 - val_mae: 21.8950\n",
      "Epoch 130/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 665.2889 - mae: 18.2848 - val_loss: 1028.7495 - val_mae: 22.1588\n",
      "Epoch 131/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 664.7759 - mae: 18.2813 - val_loss: 1016.7834 - val_mae: 21.9948\n",
      "Epoch 132/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 654.6872 - mae: 18.1331 - val_loss: 1003.8973 - val_mae: 21.5253\n",
      "Epoch 133/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 659.4816 - mae: 18.1735 - val_loss: 1010.8022 - val_mae: 21.8019\n",
      "Epoch 134/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 656.4363 - mae: 18.1650 - val_loss: 1011.9850 - val_mae: 21.8086\n",
      "Epoch 135/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 649.0984 - mae: 18.0695 - val_loss: 1016.0851 - val_mae: 21.9293\n",
      "Epoch 136/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 651.1678 - mae: 18.1064 - val_loss: 1026.8429 - val_mae: 22.0587\n",
      "Epoch 137/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 652.2825 - mae: 18.1188 - val_loss: 1029.3483 - val_mae: 22.3292\n",
      "Epoch 138/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 641.1298 - mae: 17.9995 - val_loss: 1017.5309 - val_mae: 22.0985\n",
      "Epoch 139/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 640.7475 - mae: 17.9598 - val_loss: 997.0129 - val_mae: 21.5516\n",
      "Epoch 140/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 636.2464 - mae: 17.9024 - val_loss: 989.6951 - val_mae: 21.4110\n",
      "Epoch 141/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 640.3397 - mae: 17.9710 - val_loss: 984.9326 - val_mae: 21.4375\n",
      "Epoch 142/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 632.8793 - mae: 17.8622 - val_loss: 1002.8444 - val_mae: 21.7406\n",
      "Epoch 143/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 630.3443 - mae: 17.8203 - val_loss: 981.7156 - val_mae: 21.2753\n",
      "Epoch 144/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 628.7762 - mae: 17.8307 - val_loss: 1011.2976 - val_mae: 21.8089\n",
      "Epoch 145/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 630.9775 - mae: 17.8540 - val_loss: 990.3182 - val_mae: 21.5039\n",
      "Epoch 146/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 620.9689 - mae: 17.6974 - val_loss: 984.5861 - val_mae: 21.3416\n",
      "Epoch 147/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 622.3853 - mae: 17.7400 - val_loss: 993.9536 - val_mae: 21.4558\n",
      "Epoch 148/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 615.4211 - mae: 17.6284 - val_loss: 992.7232 - val_mae: 21.5268\n",
      "Epoch 149/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 617.8298 - mae: 17.6597 - val_loss: 1000.5296 - val_mae: 21.6631\n",
      "Epoch 150/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 613.8350 - mae: 17.6131 - val_loss: 1003.6887 - val_mae: 21.4698\n",
      "Epoch 151/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 614.2078 - mae: 17.6109 - val_loss: 999.6028 - val_mae: 21.5962\n",
      "Epoch 152/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 611.4273 - mae: 17.5942 - val_loss: 1009.8519 - val_mae: 21.9970\n",
      "Epoch 153/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 609.7634 - mae: 17.5794 - val_loss: 978.0533 - val_mae: 21.2707\n",
      "Epoch 154/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 608.7109 - mae: 17.5261 - val_loss: 988.0764 - val_mae: 21.3853\n",
      "Epoch 155/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 601.9322 - mae: 17.4714 - val_loss: 1004.0787 - val_mae: 21.8590\n",
      "Epoch 156/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 600.9687 - mae: 17.4414 - val_loss: 1005.0728 - val_mae: 21.7067\n",
      "Epoch 157/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 602.7134 - mae: 17.4742 - val_loss: 971.7286 - val_mae: 21.1249\n",
      "Epoch 158/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 596.3058 - mae: 17.3790 - val_loss: 1002.1051 - val_mae: 21.6371\n",
      "Epoch 159/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 595.7332 - mae: 17.3859 - val_loss: 986.4045 - val_mae: 21.2831\n",
      "Epoch 160/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 594.3972 - mae: 17.3766 - val_loss: 986.3463 - val_mae: 21.3643\n",
      "Epoch 161/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 594.9393 - mae: 17.3790 - val_loss: 995.5327 - val_mae: 21.7938\n",
      "Epoch 162/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 586.1104 - mae: 17.2503 - val_loss: 985.0507 - val_mae: 21.4741\n",
      "Epoch 163/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 585.8777 - mae: 17.2490 - val_loss: 997.2568 - val_mae: 21.5034\n",
      "Epoch 164/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 593.8469 - mae: 17.3680 - val_loss: 979.0376 - val_mae: 21.2414\n",
      "Epoch 165/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 584.5312 - mae: 17.2216 - val_loss: 1013.3782 - val_mae: 22.1013\n",
      "Epoch 166/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 583.3727 - mae: 17.2153 - val_loss: 974.9573 - val_mae: 21.1919\n",
      "Epoch 167/1000\n",
      "438/438 [==============================] - 12s 28ms/step - loss: 578.0931 - mae: 17.1700 - val_loss: 970.0285 - val_mae: 21.1795\n",
      "Epoch 168/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 577.4417 - mae: 17.1435 - val_loss: 995.1379 - val_mae: 21.6169\n",
      "Epoch 169/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 575.3472 - mae: 17.1026 - val_loss: 966.0479 - val_mae: 21.0104\n",
      "Epoch 170/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 576.4371 - mae: 17.1083 - val_loss: 978.6691 - val_mae: 21.3134\n",
      "Epoch 171/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 570.7486 - mae: 17.0515 - val_loss: 962.0585 - val_mae: 20.9573\n",
      "Epoch 172/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 567.4468 - mae: 16.9912 - val_loss: 973.2283 - val_mae: 21.2416\n",
      "Epoch 173/1000\n",
      "438/438 [==============================] - 12s 28ms/step - loss: 568.3112 - mae: 17.0314 - val_loss: 979.9959 - val_mae: 21.2344\n",
      "Epoch 174/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 563.3258 - mae: 16.9371 - val_loss: 972.7220 - val_mae: 21.2404\n",
      "Epoch 175/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 569.8923 - mae: 17.0282 - val_loss: 970.7758 - val_mae: 21.0616\n",
      "Epoch 176/1000\n",
      "438/438 [==============================] - 12s 28ms/step - loss: 564.3914 - mae: 16.9629 - val_loss: 992.2609 - val_mae: 21.6754\n",
      "Epoch 177/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 565.1222 - mae: 16.9630 - val_loss: 968.0264 - val_mae: 21.1660\n",
      "Epoch 178/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 559.0330 - mae: 16.8749 - val_loss: 968.7510 - val_mae: 21.1189\n",
      "Epoch 179/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 559.7856 - mae: 16.8886 - val_loss: 980.6794 - val_mae: 21.2427\n",
      "Epoch 180/1000\n",
      "438/438 [==============================] - 12s 28ms/step - loss: 556.4828 - mae: 16.8356 - val_loss: 995.7024 - val_mae: 21.7818\n",
      "Epoch 181/1000\n",
      "438/438 [==============================] - 12s 28ms/step - loss: 556.9938 - mae: 16.8263 - val_loss: 955.7631 - val_mae: 20.9300\n",
      "Epoch 182/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 558.2460 - mae: 16.8589 - val_loss: 951.9281 - val_mae: 20.8396\n",
      "Epoch 183/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 549.8183 - mae: 16.7436 - val_loss: 965.9098 - val_mae: 21.1086\n",
      "Epoch 184/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 550.9001 - mae: 16.7965 - val_loss: 964.8983 - val_mae: 21.0550\n",
      "Epoch 185/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 552.3547 - mae: 16.7869 - val_loss: 1014.0081 - val_mae: 21.9521\n",
      "Epoch 186/1000\n",
      "438/438 [==============================] - 12s 29ms/step - loss: 548.2021 - mae: 16.7407 - val_loss: 990.4957 - val_mae: 21.6525\n",
      "Epoch 187/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 549.0470 - mae: 16.7296 - val_loss: 953.6177 - val_mae: 20.8482\n",
      "Epoch 188/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 543.2266 - mae: 16.6745 - val_loss: 951.4725 - val_mae: 20.8157\n",
      "Epoch 189/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 541.0457 - mae: 16.6641 - val_loss: 975.0326 - val_mae: 21.1363\n",
      "Epoch 190/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 540.5064 - mae: 16.6201 - val_loss: 950.6854 - val_mae: 20.7894\n",
      "Epoch 191/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 539.2226 - mae: 16.6183 - val_loss: 954.7502 - val_mae: 20.9187\n",
      "Epoch 192/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 534.7186 - mae: 16.5565 - val_loss: 975.6815 - val_mae: 21.2815\n",
      "Epoch 193/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 539.1841 - mae: 16.6008 - val_loss: 960.4465 - val_mae: 20.9934\n",
      "Epoch 194/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 536.8184 - mae: 16.5794 - val_loss: 969.4346 - val_mae: 21.1694\n",
      "Epoch 195/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 533.5108 - mae: 16.5240 - val_loss: 958.7009 - val_mae: 20.8738\n",
      "Epoch 196/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 536.8465 - mae: 16.5775 - val_loss: 958.0298 - val_mae: 20.9701\n",
      "Epoch 197/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 529.6023 - mae: 16.4810 - val_loss: 972.7761 - val_mae: 21.2149\n",
      "Epoch 198/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 530.5291 - mae: 16.4734 - val_loss: 1006.1587 - val_mae: 21.7546\n",
      "Epoch 199/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 531.7400 - mae: 16.5265 - val_loss: 962.2154 - val_mae: 20.9946\n",
      "Epoch 200/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 525.9324 - mae: 16.4428 - val_loss: 966.3792 - val_mae: 21.1530\n",
      "Epoch 201/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 525.4438 - mae: 16.4122 - val_loss: 955.3907 - val_mae: 20.8591\n",
      "Epoch 202/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 524.3380 - mae: 16.4014 - val_loss: 951.4827 - val_mae: 20.8567\n",
      "Epoch 203/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 522.7679 - mae: 16.3672 - val_loss: 953.5329 - val_mae: 20.8833\n",
      "Epoch 204/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 520.2577 - mae: 16.3358 - val_loss: 953.9414 - val_mae: 20.8372\n",
      "Epoch 205/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 524.5964 - mae: 16.3915 - val_loss: 957.5991 - val_mae: 20.8936\n",
      "Epoch 206/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 521.3242 - mae: 16.3315 - val_loss: 953.0295 - val_mae: 20.7613\n",
      "Epoch 207/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 517.1687 - mae: 16.3106 - val_loss: 966.4033 - val_mae: 21.1048\n",
      "Epoch 208/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 516.3595 - mae: 16.3030 - val_loss: 945.1634 - val_mae: 20.8303\n",
      "Epoch 209/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 514.9947 - mae: 16.2619 - val_loss: 953.5435 - val_mae: 20.8585\n",
      "Epoch 210/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 514.0500 - mae: 16.2614 - val_loss: 972.2631 - val_mae: 21.2620\n",
      "Epoch 211/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 514.1279 - mae: 16.2621 - val_loss: 951.9799 - val_mae: 20.9108\n",
      "Epoch 212/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 512.0995 - mae: 16.2181 - val_loss: 955.6510 - val_mae: 20.7810\n",
      "Epoch 213/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 510.6690 - mae: 16.2224 - val_loss: 940.9386 - val_mae: 20.6883\n",
      "Epoch 214/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 510.0125 - mae: 16.1834 - val_loss: 950.8436 - val_mae: 20.7898\n",
      "Epoch 215/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 507.5924 - mae: 16.1482 - val_loss: 950.7038 - val_mae: 20.7960\n",
      "Epoch 216/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 509.9041 - mae: 16.2040 - val_loss: 963.9239 - val_mae: 21.1493\n",
      "Epoch 217/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 504.6563 - mae: 16.1132 - val_loss: 964.5987 - val_mae: 21.0993\n",
      "Epoch 218/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 505.7295 - mae: 16.1261 - val_loss: 957.8644 - val_mae: 20.9825\n",
      "Epoch 219/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 502.8106 - mae: 16.0944 - val_loss: 962.2175 - val_mae: 21.1220\n",
      "Epoch 220/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 501.2574 - mae: 16.0803 - val_loss: 950.9849 - val_mae: 20.8856\n",
      "Epoch 221/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 502.6129 - mae: 16.0969 - val_loss: 950.6003 - val_mae: 20.8881\n",
      "Epoch 222/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 499.8133 - mae: 16.0872 - val_loss: 973.1442 - val_mae: 21.2337\n",
      "Epoch 223/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 500.1918 - mae: 16.0584 - val_loss: 954.7451 - val_mae: 20.8549\n",
      "Epoch 224/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 496.6721 - mae: 15.9934 - val_loss: 950.6440 - val_mae: 20.8765\n",
      "Epoch 225/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 498.5348 - mae: 16.0282 - val_loss: 957.4581 - val_mae: 20.9681\n",
      "Epoch 226/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 497.5222 - mae: 16.0040 - val_loss: 948.9641 - val_mae: 20.9143\n",
      "Epoch 227/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 495.3510 - mae: 15.9586 - val_loss: 959.1788 - val_mae: 21.1239\n",
      "Epoch 228/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 492.4835 - mae: 15.9212 - val_loss: 947.9040 - val_mae: 20.6923\n",
      "Epoch 229/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 492.5589 - mae: 15.9378 - val_loss: 974.0817 - val_mae: 21.3152\n",
      "Epoch 230/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 490.4778 - mae: 15.9095 - val_loss: 938.0882 - val_mae: 20.6824\n",
      "Epoch 231/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 491.5892 - mae: 15.9352 - val_loss: 947.1207 - val_mae: 20.7220\n",
      "Epoch 232/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 484.0217 - mae: 15.8488 - val_loss: 946.1951 - val_mae: 20.7007\n",
      "Epoch 233/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 488.9556 - mae: 15.8722 - val_loss: 960.2363 - val_mae: 20.9711\n",
      "Epoch 234/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 487.2747 - mae: 15.8695 - val_loss: 951.8557 - val_mae: 20.8960\n",
      "Epoch 235/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 485.3161 - mae: 15.8391 - val_loss: 941.2433 - val_mae: 20.7091\n",
      "Epoch 236/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 483.6756 - mae: 15.7992 - val_loss: 944.1767 - val_mae: 20.6266\n",
      "Epoch 237/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 486.4179 - mae: 15.8545 - val_loss: 948.0745 - val_mae: 20.7900\n",
      "Epoch 238/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 481.0125 - mae: 15.7771 - val_loss: 941.9526 - val_mae: 20.6633\n",
      "Epoch 239/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 483.8317 - mae: 15.8154 - val_loss: 940.6378 - val_mae: 20.6803\n",
      "Epoch 240/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 481.6805 - mae: 15.7980 - val_loss: 952.4949 - val_mae: 20.7743\n",
      "Epoch 241/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 477.4954 - mae: 15.7132 - val_loss: 961.9864 - val_mae: 21.0056\n",
      "Epoch 242/1000\n",
      "438/438 [==============================] - 15s 35ms/step - loss: 479.1044 - mae: 15.7485 - val_loss: 965.3596 - val_mae: 21.1075\n",
      "Epoch 243/1000\n",
      "438/438 [==============================] - 15s 35ms/step - loss: 476.5319 - mae: 15.7086 - val_loss: 961.1557 - val_mae: 21.0672\n",
      "Epoch 244/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 475.4468 - mae: 15.6673 - val_loss: 943.9323 - val_mae: 20.7246\n",
      "Epoch 245/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 477.4307 - mae: 15.7200 - val_loss: 942.1376 - val_mae: 20.6359\n",
      "Epoch 246/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 474.2154 - mae: 15.6561 - val_loss: 947.5248 - val_mae: 20.7009\n",
      "Epoch 247/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 475.3090 - mae: 15.6971 - val_loss: 941.4069 - val_mae: 20.5979\n",
      "Epoch 248/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 474.0055 - mae: 15.6660 - val_loss: 949.6033 - val_mae: 20.7631\n",
      "Epoch 249/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 473.2377 - mae: 15.6678 - val_loss: 927.9066 - val_mae: 20.5751\n",
      "Epoch 250/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 474.6138 - mae: 15.6629 - val_loss: 941.0245 - val_mae: 20.5601\n",
      "Epoch 251/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 475.0659 - mae: 15.6505 - val_loss: 954.7051 - val_mae: 20.9353\n",
      "Epoch 252/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 470.1353 - mae: 15.6053 - val_loss: 948.6545 - val_mae: 20.8202\n",
      "Epoch 253/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 469.2509 - mae: 15.5787 - val_loss: 940.6927 - val_mae: 20.6428\n",
      "Epoch 254/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 468.5983 - mae: 15.5769 - val_loss: 944.3925 - val_mae: 20.6322\n",
      "Epoch 255/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 467.6469 - mae: 15.5726 - val_loss: 960.6426 - val_mae: 21.1402\n",
      "Epoch 256/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 467.6508 - mae: 15.5971 - val_loss: 944.2864 - val_mae: 20.5995\n",
      "Epoch 257/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 467.5245 - mae: 15.5585 - val_loss: 942.7070 - val_mae: 20.5906\n",
      "Epoch 258/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 463.5586 - mae: 15.5071 - val_loss: 962.3394 - val_mae: 21.0407\n",
      "Epoch 259/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 463.8405 - mae: 15.5050 - val_loss: 931.2891 - val_mae: 20.4343\n",
      "Epoch 260/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 462.7531 - mae: 15.4955 - val_loss: 958.9335 - val_mae: 21.0456\n",
      "Epoch 261/1000\n",
      "438/438 [==============================] - 15s 34ms/step - loss: 461.3449 - mae: 15.4686 - val_loss: 934.3199 - val_mae: 20.5301\n",
      "Epoch 262/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 460.2134 - mae: 15.4727 - val_loss: 961.6739 - val_mae: 21.0883\n",
      "Epoch 263/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 458.1913 - mae: 15.4398 - val_loss: 940.3847 - val_mae: 20.6003\n",
      "Epoch 264/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 459.6679 - mae: 15.4409 - val_loss: 937.7736 - val_mae: 20.5425\n",
      "Epoch 265/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 459.7290 - mae: 15.4468 - val_loss: 943.8177 - val_mae: 20.5386\n",
      "Epoch 266/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 458.3779 - mae: 15.4189 - val_loss: 936.1248 - val_mae: 20.5420\n",
      "Epoch 267/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 458.2334 - mae: 15.4114 - val_loss: 935.0692 - val_mae: 20.5184\n",
      "Epoch 268/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 460.3050 - mae: 15.4450 - val_loss: 945.6500 - val_mae: 20.6976\n",
      "Epoch 269/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 457.0882 - mae: 15.4192 - val_loss: 940.9564 - val_mae: 20.6453\n",
      "Epoch 270/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 454.9094 - mae: 15.3626 - val_loss: 936.4823 - val_mae: 20.6934\n",
      "Epoch 271/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 453.2983 - mae: 15.3344 - val_loss: 938.0135 - val_mae: 20.5618\n",
      "Epoch 272/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 453.2602 - mae: 15.3374 - val_loss: 951.6179 - val_mae: 20.8215\n",
      "Epoch 273/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 454.4086 - mae: 15.3562 - val_loss: 942.4778 - val_mae: 20.7276\n",
      "Epoch 274/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 451.7419 - mae: 15.3432 - val_loss: 941.8591 - val_mae: 20.5589\n",
      "Epoch 275/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 453.6263 - mae: 15.3458 - val_loss: 943.1212 - val_mae: 20.6057\n",
      "Epoch 276/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 449.8744 - mae: 15.2906 - val_loss: 940.7069 - val_mae: 20.6084\n",
      "Epoch 277/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 451.4943 - mae: 15.3127 - val_loss: 933.0899 - val_mae: 20.5885\n",
      "Epoch 278/1000\n",
      "438/438 [==============================] - 15s 34ms/step - loss: 447.1234 - mae: 15.2922 - val_loss: 936.8754 - val_mae: 20.6426\n",
      "Epoch 279/1000\n",
      "438/438 [==============================] - 15s 34ms/step - loss: 450.9338 - mae: 15.2962 - val_loss: 950.8885 - val_mae: 20.8991\n",
      "Epoch 280/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 446.3540 - mae: 15.2482 - val_loss: 938.4627 - val_mae: 20.5515\n",
      "Epoch 281/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 449.1026 - mae: 15.2764 - val_loss: 938.1405 - val_mae: 20.5593\n",
      "Epoch 282/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 444.7244 - mae: 15.2164 - val_loss: 963.2670 - val_mae: 21.0342\n",
      "Epoch 283/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 447.8360 - mae: 15.2725 - val_loss: 943.6734 - val_mae: 20.6811\n",
      "Epoch 284/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 447.0093 - mae: 15.2758 - val_loss: 963.8165 - val_mae: 21.0670\n",
      "Epoch 285/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 445.9977 - mae: 15.2540 - val_loss: 932.4033 - val_mae: 20.5083\n",
      "Epoch 286/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 443.8942 - mae: 15.1997 - val_loss: 943.0803 - val_mae: 20.6100\n",
      "Epoch 287/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 442.0361 - mae: 15.1717 - val_loss: 939.6953 - val_mae: 20.7495\n",
      "Epoch 288/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 439.2451 - mae: 15.1441 - val_loss: 939.3758 - val_mae: 20.4641\n",
      "Epoch 289/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 440.1946 - mae: 15.1390 - val_loss: 932.3539 - val_mae: 20.4164\n",
      "Epoch 290/1000\n",
      "438/438 [==============================] - 13s 29ms/step - loss: 442.1940 - mae: 15.1815 - val_loss: 931.3455 - val_mae: 20.4776\n",
      "Epoch 291/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 438.0808 - mae: 15.1219 - val_loss: 941.3630 - val_mae: 20.6884\n",
      "Epoch 292/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 441.7539 - mae: 15.1803 - val_loss: 937.0852 - val_mae: 20.5400\n",
      "Epoch 293/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 438.6714 - mae: 15.1230 - val_loss: 939.2017 - val_mae: 20.6493\n",
      "Epoch 294/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 443.1856 - mae: 15.1884 - val_loss: 944.8469 - val_mae: 20.7568\n",
      "Epoch 295/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 436.7082 - mae: 15.0845 - val_loss: 953.9118 - val_mae: 20.7709\n",
      "Epoch 296/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 436.8329 - mae: 15.0526 - val_loss: 946.4173 - val_mae: 20.8050\n",
      "Epoch 297/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 438.7779 - mae: 15.1181 - val_loss: 940.0117 - val_mae: 20.7418\n",
      "Epoch 298/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 439.9087 - mae: 15.1307 - val_loss: 942.2072 - val_mae: 20.6120\n",
      "Epoch 299/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 438.5481 - mae: 15.1116 - val_loss: 946.7104 - val_mae: 20.5792\n",
      "Epoch 300/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 434.7212 - mae: 15.0550 - val_loss: 944.6313 - val_mae: 20.6321\n",
      "Epoch 301/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 436.1224 - mae: 15.0968 - val_loss: 933.4006 - val_mae: 20.4849\n",
      "Epoch 302/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 433.4741 - mae: 15.0424 - val_loss: 936.1810 - val_mae: 20.5417\n",
      "Epoch 303/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 433.5584 - mae: 15.0366 - val_loss: 943.5125 - val_mae: 20.6444\n",
      "Epoch 304/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 433.0914 - mae: 15.0239 - val_loss: 950.8206 - val_mae: 20.6555\n",
      "Epoch 305/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 432.5948 - mae: 15.0256 - val_loss: 936.1284 - val_mae: 20.6073\n",
      "Epoch 306/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 430.4788 - mae: 14.9920 - val_loss: 932.7573 - val_mae: 20.4275\n",
      "Epoch 307/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 429.7413 - mae: 14.9700 - val_loss: 936.8975 - val_mae: 20.5178\n",
      "Epoch 308/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 429.1501 - mae: 14.9696 - val_loss: 945.0068 - val_mae: 20.6408\n",
      "Epoch 309/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 430.9918 - mae: 15.0090 - val_loss: 927.5072 - val_mae: 20.3489\n",
      "Epoch 310/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 427.1142 - mae: 14.9305 - val_loss: 938.6320 - val_mae: 20.6489\n",
      "Epoch 311/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 433.6628 - mae: 15.0345 - val_loss: 934.8185 - val_mae: 20.5150\n",
      "Epoch 312/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 426.4979 - mae: 14.9113 - val_loss: 934.4127 - val_mae: 20.4718\n",
      "Epoch 313/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 427.1866 - mae: 14.9461 - val_loss: 931.7924 - val_mae: 20.4226\n",
      "Epoch 314/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 427.1101 - mae: 14.9307 - val_loss: 930.8951 - val_mae: 20.4307\n",
      "Epoch 315/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 426.2260 - mae: 14.9160 - val_loss: 927.1650 - val_mae: 20.3993\n",
      "Epoch 316/1000\n",
      "438/438 [==============================] - 15s 34ms/step - loss: 423.7753 - mae: 14.9042 - val_loss: 938.7762 - val_mae: 20.4867\n",
      "Epoch 317/1000\n",
      "438/438 [==============================] - 16s 37ms/step - loss: 426.7077 - mae: 14.9429 - val_loss: 930.2435 - val_mae: 20.4329\n",
      "Epoch 318/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 419.3084 - mae: 14.7988 - val_loss: 932.2836 - val_mae: 20.5168\n",
      "Epoch 319/1000\n",
      "438/438 [==============================] - 15s 33ms/step - loss: 425.6075 - mae: 14.9306 - val_loss: 933.2372 - val_mae: 20.5340\n",
      "Epoch 320/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 424.2355 - mae: 14.8673 - val_loss: 937.4882 - val_mae: 20.5797\n",
      "Epoch 321/1000\n",
      "438/438 [==============================] - 15s 33ms/step - loss: 421.2024 - mae: 14.8303 - val_loss: 935.0418 - val_mae: 20.4623\n",
      "Epoch 322/1000\n",
      "438/438 [==============================] - 15s 34ms/step - loss: 425.5961 - mae: 14.8938 - val_loss: 953.3700 - val_mae: 20.8581\n",
      "Epoch 323/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 427.7962 - mae: 14.9223 - val_loss: 930.6011 - val_mae: 20.5086\n",
      "Epoch 324/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 421.5079 - mae: 14.8405 - val_loss: 933.7296 - val_mae: 20.4884\n",
      "Epoch 325/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 419.2011 - mae: 14.8072 - val_loss: 939.4220 - val_mae: 20.5009\n",
      "Epoch 326/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 422.9793 - mae: 14.8564 - val_loss: 934.9573 - val_mae: 20.5170\n",
      "Epoch 327/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 421.7530 - mae: 14.8357 - val_loss: 931.4376 - val_mae: 20.4764\n",
      "Epoch 328/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 418.9394 - mae: 14.7892 - val_loss: 938.8348 - val_mae: 20.6399\n",
      "Epoch 329/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 418.7448 - mae: 14.7710 - val_loss: 927.0229 - val_mae: 20.4064\n",
      "Epoch 330/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 414.0933 - mae: 14.7175 - val_loss: 932.1650 - val_mae: 20.4262\n",
      "Epoch 331/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 415.3527 - mae: 14.7605 - val_loss: 944.3145 - val_mae: 20.7004\n",
      "Epoch 332/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 416.8948 - mae: 14.7691 - val_loss: 933.1661 - val_mae: 20.5677\n",
      "Epoch 333/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 415.5666 - mae: 14.7778 - val_loss: 951.0098 - val_mae: 20.7753\n",
      "Epoch 334/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 416.7801 - mae: 14.7677 - val_loss: 937.6494 - val_mae: 20.5042\n",
      "Epoch 335/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 415.5391 - mae: 14.7632 - val_loss: 938.9162 - val_mae: 20.4539\n",
      "Epoch 336/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 416.5624 - mae: 14.7594 - val_loss: 934.4403 - val_mae: 20.4215\n",
      "Epoch 337/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 415.1361 - mae: 14.7386 - val_loss: 935.6221 - val_mae: 20.4842\n",
      "Epoch 338/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 415.5342 - mae: 14.7660 - val_loss: 938.2992 - val_mae: 20.4471\n",
      "Epoch 339/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 417.2675 - mae: 14.7921 - val_loss: 925.9222 - val_mae: 20.3452\n",
      "Epoch 340/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 410.7126 - mae: 14.6646 - val_loss: 928.0713 - val_mae: 20.3632\n",
      "Epoch 341/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 412.4380 - mae: 14.6993 - val_loss: 942.9778 - val_mae: 20.6116\n",
      "Epoch 342/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 412.4630 - mae: 14.7099 - val_loss: 931.1195 - val_mae: 20.3421\n",
      "Epoch 343/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 413.3943 - mae: 14.7205 - val_loss: 929.9912 - val_mae: 20.4088\n",
      "Epoch 344/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 409.0848 - mae: 14.6523 - val_loss: 932.5800 - val_mae: 20.4600\n",
      "Epoch 345/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 411.3748 - mae: 14.6851 - val_loss: 933.1987 - val_mae: 20.5625\n",
      "Epoch 346/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 410.3260 - mae: 14.6637 - val_loss: 934.4996 - val_mae: 20.4172\n",
      "Epoch 347/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 409.2861 - mae: 14.6345 - val_loss: 928.5518 - val_mae: 20.4391\n",
      "Epoch 348/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 411.3557 - mae: 14.6627 - val_loss: 935.6848 - val_mae: 20.4227\n",
      "Epoch 349/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 407.6874 - mae: 14.6408 - val_loss: 930.2846 - val_mae: 20.4367\n",
      "Epoch 350/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 412.3341 - mae: 14.6865 - val_loss: 931.6814 - val_mae: 20.4733\n",
      "Epoch 351/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 403.2281 - mae: 14.5580 - val_loss: 935.5848 - val_mae: 20.4933\n",
      "Epoch 352/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 409.5848 - mae: 14.6501 - val_loss: 934.2761 - val_mae: 20.5022\n",
      "Epoch 353/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 405.8939 - mae: 14.5868 - val_loss: 948.9055 - val_mae: 20.7536\n",
      "Epoch 354/1000\n",
      "438/438 [==============================] - 15s 34ms/step - loss: 407.1413 - mae: 14.6331 - val_loss: 930.5978 - val_mae: 20.4021\n",
      "Epoch 355/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 405.5587 - mae: 14.5879 - val_loss: 929.6467 - val_mae: 20.4063\n",
      "Epoch 356/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 408.3844 - mae: 14.6453 - val_loss: 937.4953 - val_mae: 20.4610\n",
      "Epoch 357/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 405.9088 - mae: 14.5785 - val_loss: 940.2941 - val_mae: 20.6064\n",
      "Epoch 358/1000\n",
      "438/438 [==============================] - 15s 34ms/step - loss: 405.5636 - mae: 14.5794 - val_loss: 927.5646 - val_mae: 20.3896\n",
      "Epoch 359/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 402.1685 - mae: 14.5263 - val_loss: 937.9369 - val_mae: 20.4544\n",
      "Epoch 360/1000\n",
      "438/438 [==============================] - 15s 34ms/step - loss: 407.2610 - mae: 14.6204 - val_loss: 942.5073 - val_mae: 20.4721\n",
      "Epoch 361/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 400.1714 - mae: 14.5193 - val_loss: 937.9667 - val_mae: 20.5262\n",
      "Epoch 362/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 400.3714 - mae: 14.5108 - val_loss: 939.0950 - val_mae: 20.4231\n",
      "Epoch 363/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 403.8419 - mae: 14.5383 - val_loss: 939.6738 - val_mae: 20.5572\n",
      "Epoch 364/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 405.2980 - mae: 14.5927 - val_loss: 934.6005 - val_mae: 20.5548\n",
      "Epoch 365/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 403.0385 - mae: 14.5451 - val_loss: 928.7493 - val_mae: 20.4630\n",
      "Epoch 366/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 405.1732 - mae: 14.5732 - val_loss: 944.0847 - val_mae: 20.5511\n",
      "Epoch 367/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 400.8750 - mae: 14.4932 - val_loss: 934.6859 - val_mae: 20.4641\n",
      "Epoch 368/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 406.0741 - mae: 14.5904 - val_loss: 922.8583 - val_mae: 20.2718\n",
      "Epoch 369/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 401.1509 - mae: 14.5077 - val_loss: 938.1744 - val_mae: 20.4660\n",
      "Epoch 370/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 400.5895 - mae: 14.4996 - val_loss: 931.3835 - val_mae: 20.3823\n",
      "Epoch 371/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 397.8068 - mae: 14.4511 - val_loss: 937.5634 - val_mae: 20.5182\n",
      "Epoch 372/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 397.7870 - mae: 14.4697 - val_loss: 922.6625 - val_mae: 20.2549\n",
      "Epoch 373/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 402.4980 - mae: 14.5270 - val_loss: 924.2216 - val_mae: 20.2596\n",
      "Epoch 374/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 398.7305 - mae: 14.4965 - val_loss: 930.6103 - val_mae: 20.3597\n",
      "Epoch 375/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 397.8335 - mae: 14.4621 - val_loss: 940.3071 - val_mae: 20.4693\n",
      "Epoch 376/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 402.2304 - mae: 14.5296 - val_loss: 928.3450 - val_mae: 20.3691\n",
      "Epoch 377/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 400.5435 - mae: 14.4783 - val_loss: 933.9077 - val_mae: 20.3972\n",
      "Epoch 378/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 397.5903 - mae: 14.4485 - val_loss: 929.8165 - val_mae: 20.4050\n",
      "Epoch 379/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 398.2532 - mae: 14.4489 - val_loss: 936.8807 - val_mae: 20.4126\n",
      "Epoch 380/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 396.9650 - mae: 14.4388 - val_loss: 934.8265 - val_mae: 20.3575\n",
      "Epoch 381/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 398.7980 - mae: 14.4691 - val_loss: 928.3757 - val_mae: 20.3201\n",
      "Epoch 382/1000\n",
      "438/438 [==============================] - 15s 33ms/step - loss: 400.0615 - mae: 14.4862 - val_loss: 934.3287 - val_mae: 20.3631\n",
      "Epoch 383/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 394.9398 - mae: 14.4037 - val_loss: 932.9050 - val_mae: 20.4456\n",
      "Epoch 384/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 393.9675 - mae: 14.4077 - val_loss: 927.1585 - val_mae: 20.3317\n",
      "Epoch 385/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 393.2991 - mae: 14.3932 - val_loss: 927.8999 - val_mae: 20.2846\n",
      "Epoch 386/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 394.2579 - mae: 14.3784 - val_loss: 927.2266 - val_mae: 20.2608\n",
      "Epoch 387/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 395.9952 - mae: 14.4219 - val_loss: 929.0894 - val_mae: 20.3004\n",
      "Epoch 388/1000\n",
      "438/438 [==============================] - 15s 33ms/step - loss: 393.3263 - mae: 14.4025 - val_loss: 925.8816 - val_mae: 20.2573\n",
      "Epoch 389/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 390.5388 - mae: 14.3648 - val_loss: 935.3434 - val_mae: 20.4225\n",
      "Epoch 390/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 396.3758 - mae: 14.4324 - val_loss: 930.3270 - val_mae: 20.3102\n",
      "Epoch 391/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 391.5297 - mae: 14.3472 - val_loss: 930.7993 - val_mae: 20.4257\n",
      "Epoch 392/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 393.9485 - mae: 14.3807 - val_loss: 926.3663 - val_mae: 20.2967\n",
      "Epoch 393/1000\n",
      "438/438 [==============================] - 15s 34ms/step - loss: 389.7920 - mae: 14.3292 - val_loss: 930.6743 - val_mae: 20.4280\n",
      "Epoch 394/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 392.5080 - mae: 14.3773 - val_loss: 931.7012 - val_mae: 20.3441\n",
      "Epoch 395/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 390.1668 - mae: 14.3380 - val_loss: 925.2398 - val_mae: 20.2946\n",
      "Epoch 396/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 389.5916 - mae: 14.3281 - val_loss: 931.6711 - val_mae: 20.3576\n",
      "Epoch 397/1000\n",
      "438/438 [==============================] - 15s 33ms/step - loss: 387.6661 - mae: 14.2873 - val_loss: 949.0675 - val_mae: 20.7629\n",
      "Epoch 398/1000\n",
      "438/438 [==============================] - 15s 34ms/step - loss: 389.1340 - mae: 14.3315 - val_loss: 926.0637 - val_mae: 20.2972\n",
      "Epoch 399/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 388.4219 - mae: 14.3050 - val_loss: 924.0577 - val_mae: 20.2899\n",
      "Epoch 400/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 386.8239 - mae: 14.2836 - val_loss: 930.0822 - val_mae: 20.4632\n",
      "Epoch 401/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 387.6899 - mae: 14.2952 - val_loss: 927.0197 - val_mae: 20.3420\n",
      "Epoch 402/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 386.7836 - mae: 14.2791 - val_loss: 933.5779 - val_mae: 20.5277\n",
      "Epoch 403/1000\n",
      "438/438 [==============================] - 15s 33ms/step - loss: 386.8538 - mae: 14.2780 - val_loss: 926.6839 - val_mae: 20.3447\n",
      "Epoch 404/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 386.5691 - mae: 14.2745 - val_loss: 927.6779 - val_mae: 20.3700\n",
      "Epoch 405/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 385.9009 - mae: 14.2633 - val_loss: 928.0091 - val_mae: 20.3243\n",
      "Epoch 406/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 384.1360 - mae: 14.2420 - val_loss: 924.8961 - val_mae: 20.3159\n",
      "Epoch 407/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 386.2848 - mae: 14.2645 - val_loss: 922.6039 - val_mae: 20.2531\n",
      "Epoch 408/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 385.6805 - mae: 14.2611 - val_loss: 924.2528 - val_mae: 20.2890\n",
      "Epoch 409/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 384.5116 - mae: 14.2321 - val_loss: 929.2945 - val_mae: 20.3817\n",
      "Epoch 410/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 383.0983 - mae: 14.2289 - val_loss: 930.1166 - val_mae: 20.2942\n",
      "Epoch 411/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 386.0272 - mae: 14.2528 - val_loss: 937.8839 - val_mae: 20.3703\n",
      "Epoch 412/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 385.9964 - mae: 14.2709 - val_loss: 929.9839 - val_mae: 20.2821\n",
      "Epoch 413/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 384.6973 - mae: 14.2642 - val_loss: 924.8205 - val_mae: 20.2738\n",
      "Epoch 414/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 383.1430 - mae: 14.1963 - val_loss: 919.4862 - val_mae: 20.2101\n",
      "Epoch 415/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 382.7911 - mae: 14.2081 - val_loss: 933.0994 - val_mae: 20.3525\n",
      "Epoch 416/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 384.8844 - mae: 14.2394 - val_loss: 930.5763 - val_mae: 20.3255\n",
      "Epoch 417/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 385.9017 - mae: 14.2555 - val_loss: 928.5585 - val_mae: 20.3609\n",
      "Epoch 418/1000\n",
      "438/438 [==============================] - 15s 34ms/step - loss: 382.8134 - mae: 14.1960 - val_loss: 924.9853 - val_mae: 20.2819\n",
      "Epoch 419/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 382.8459 - mae: 14.2005 - val_loss: 927.5559 - val_mae: 20.2694\n",
      "Epoch 420/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 379.2862 - mae: 14.1377 - val_loss: 927.2542 - val_mae: 20.3033\n",
      "Epoch 421/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 382.4054 - mae: 14.2076 - val_loss: 934.5759 - val_mae: 20.4101\n",
      "Epoch 422/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 379.4263 - mae: 14.1625 - val_loss: 927.2133 - val_mae: 20.2606\n",
      "Epoch 423/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 383.3404 - mae: 14.2124 - val_loss: 934.8978 - val_mae: 20.3616\n",
      "Epoch 424/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 378.5392 - mae: 14.1329 - val_loss: 929.7849 - val_mae: 20.4214\n",
      "Epoch 425/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 382.0930 - mae: 14.2024 - val_loss: 927.6823 - val_mae: 20.2882\n",
      "Epoch 426/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 382.3956 - mae: 14.1873 - val_loss: 940.8627 - val_mae: 20.4890\n",
      "Epoch 427/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 379.6862 - mae: 14.1611 - val_loss: 934.2513 - val_mae: 20.4994\n",
      "Epoch 428/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 377.1793 - mae: 14.0925 - val_loss: 921.3976 - val_mae: 20.2553\n",
      "Epoch 429/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 380.6466 - mae: 14.1865 - val_loss: 931.2899 - val_mae: 20.3222\n",
      "Epoch 430/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 377.1109 - mae: 14.1178 - val_loss: 924.2219 - val_mae: 20.2601\n",
      "Epoch 431/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 375.4740 - mae: 14.0876 - val_loss: 929.0491 - val_mae: 20.3242\n",
      "Epoch 432/1000\n",
      "438/438 [==============================] - 15s 34ms/step - loss: 374.3290 - mae: 14.0655 - val_loss: 922.4404 - val_mae: 20.3133\n",
      "Epoch 433/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 378.4023 - mae: 14.1243 - val_loss: 917.8499 - val_mae: 20.1383\n",
      "Epoch 434/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 380.2599 - mae: 14.1645 - val_loss: 922.8676 - val_mae: 20.2374\n",
      "Epoch 435/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 376.6707 - mae: 14.1294 - val_loss: 926.7267 - val_mae: 20.3551\n",
      "Epoch 436/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 374.8672 - mae: 14.0696 - val_loss: 929.0400 - val_mae: 20.3190\n",
      "Epoch 437/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 376.3305 - mae: 14.1105 - val_loss: 922.0611 - val_mae: 20.2333\n",
      "Epoch 438/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 373.0758 - mae: 14.0526 - val_loss: 921.5441 - val_mae: 20.2166\n",
      "Epoch 439/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 375.9424 - mae: 14.1028 - val_loss: 920.4802 - val_mae: 20.2374\n",
      "Epoch 440/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 375.0202 - mae: 14.0462 - val_loss: 922.9879 - val_mae: 20.3069\n",
      "Epoch 441/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 375.0479 - mae: 14.0638 - val_loss: 924.3765 - val_mae: 20.2775\n",
      "Epoch 442/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 374.0417 - mae: 14.0427 - val_loss: 922.7632 - val_mae: 20.2956\n",
      "Epoch 443/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 376.4115 - mae: 14.0896 - val_loss: 918.5316 - val_mae: 20.1708\n",
      "Epoch 444/1000\n",
      "438/438 [==============================] - 15s 34ms/step - loss: 374.6370 - mae: 14.0540 - val_loss: 936.8488 - val_mae: 20.4829\n",
      "Epoch 445/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 373.4912 - mae: 14.0257 - val_loss: 928.0244 - val_mae: 20.3052\n",
      "Epoch 446/1000\n",
      "438/438 [==============================] - 15s 33ms/step - loss: 373.5401 - mae: 14.0409 - val_loss: 936.4551 - val_mae: 20.4769\n",
      "Epoch 447/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 369.5725 - mae: 13.9949 - val_loss: 912.9899 - val_mae: 20.1271\n",
      "Epoch 448/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 372.9088 - mae: 14.0429 - val_loss: 924.5167 - val_mae: 20.1790\n",
      "Epoch 449/1000\n",
      "438/438 [==============================] - 15s 34ms/step - loss: 372.0312 - mae: 14.0211 - val_loss: 918.0242 - val_mae: 20.2187\n",
      "Epoch 450/1000\n",
      "438/438 [==============================] - 15s 33ms/step - loss: 371.6086 - mae: 14.0161 - val_loss: 923.5734 - val_mae: 20.2328\n",
      "Epoch 451/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 372.8118 - mae: 14.0052 - val_loss: 931.4459 - val_mae: 20.4296\n",
      "Epoch 452/1000\n",
      "438/438 [==============================] - 15s 33ms/step - loss: 372.1286 - mae: 14.0043 - val_loss: 924.8954 - val_mae: 20.3809\n",
      "Epoch 453/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 372.5710 - mae: 14.0233 - val_loss: 920.1815 - val_mae: 20.1532\n",
      "Epoch 454/1000\n",
      "438/438 [==============================] - 15s 33ms/step - loss: 374.6380 - mae: 14.0673 - val_loss: 919.3915 - val_mae: 20.2363\n",
      "Epoch 455/1000\n",
      "438/438 [==============================] - 15s 35ms/step - loss: 372.0898 - mae: 14.0302 - val_loss: 918.5980 - val_mae: 20.2143\n",
      "Epoch 456/1000\n",
      "438/438 [==============================] - 15s 35ms/step - loss: 371.3772 - mae: 13.9905 - val_loss: 914.2529 - val_mae: 20.1839\n",
      "Epoch 457/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 369.8127 - mae: 13.9960 - val_loss: 925.4270 - val_mae: 20.2417\n",
      "Epoch 458/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 368.3965 - mae: 13.9828 - val_loss: 926.7205 - val_mae: 20.3517\n",
      "Epoch 459/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 366.7500 - mae: 13.9315 - val_loss: 916.3942 - val_mae: 20.1546\n",
      "Epoch 460/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 368.5020 - mae: 13.9643 - val_loss: 927.4056 - val_mae: 20.2220\n",
      "Epoch 461/1000\n",
      "438/438 [==============================] - 15s 34ms/step - loss: 368.3944 - mae: 13.9655 - val_loss: 925.5463 - val_mae: 20.2399\n",
      "Epoch 462/1000\n",
      "438/438 [==============================] - 15s 34ms/step - loss: 370.5975 - mae: 14.0063 - val_loss: 921.1716 - val_mae: 20.2096\n",
      "Epoch 463/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 371.1083 - mae: 13.9992 - val_loss: 931.1076 - val_mae: 20.3861\n",
      "Epoch 464/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 369.5580 - mae: 13.9812 - val_loss: 929.3301 - val_mae: 20.3022\n",
      "Epoch 465/1000\n",
      "438/438 [==============================] - 16s 36ms/step - loss: 368.8831 - mae: 13.9727 - val_loss: 927.5630 - val_mae: 20.3599\n",
      "Epoch 466/1000\n",
      "438/438 [==============================] - 15s 34ms/step - loss: 364.6656 - mae: 13.9139 - val_loss: 930.9963 - val_mae: 20.3991\n",
      "Epoch 467/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 367.1024 - mae: 13.9374 - val_loss: 925.9919 - val_mae: 20.2863\n",
      "Epoch 468/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 365.1562 - mae: 13.8982 - val_loss: 923.9008 - val_mae: 20.3687\n",
      "Epoch 469/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 368.3642 - mae: 13.9620 - val_loss: 922.7892 - val_mae: 20.2101\n",
      "Epoch 470/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 368.4530 - mae: 13.9715 - val_loss: 920.8279 - val_mae: 20.1967\n",
      "Epoch 471/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 367.8054 - mae: 13.9414 - val_loss: 925.7826 - val_mae: 20.2562\n",
      "Epoch 472/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 367.3995 - mae: 13.9325 - val_loss: 922.2948 - val_mae: 20.2032\n",
      "Epoch 473/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 368.5052 - mae: 13.9714 - val_loss: 928.0647 - val_mae: 20.2732\n",
      "Epoch 474/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 366.3358 - mae: 13.9287 - val_loss: 919.5699 - val_mae: 20.1856\n",
      "Epoch 475/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 367.1994 - mae: 13.9166 - val_loss: 925.2849 - val_mae: 20.2080\n",
      "Epoch 476/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 364.5107 - mae: 13.8819 - val_loss: 923.5533 - val_mae: 20.2927\n",
      "Epoch 477/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 366.0389 - mae: 13.9118 - val_loss: 921.8931 - val_mae: 20.2218\n",
      "Epoch 478/1000\n",
      "438/438 [==============================] - 15s 34ms/step - loss: 364.9356 - mae: 13.9003 - val_loss: 925.5600 - val_mae: 20.2916\n",
      "Epoch 479/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 367.2292 - mae: 13.9358 - val_loss: 928.1520 - val_mae: 20.2656\n",
      "Epoch 480/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 361.2827 - mae: 13.8450 - val_loss: 922.5424 - val_mae: 20.2124\n",
      "Epoch 481/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 365.0249 - mae: 13.9069 - val_loss: 918.2429 - val_mae: 20.1848\n",
      "Epoch 482/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 365.9337 - mae: 13.9093 - val_loss: 933.0370 - val_mae: 20.4569\n",
      "Epoch 483/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 364.7107 - mae: 13.8984 - val_loss: 929.6694 - val_mae: 20.4096\n",
      "Epoch 484/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 365.1590 - mae: 13.8963 - val_loss: 927.4590 - val_mae: 20.2418\n",
      "Epoch 485/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 364.4446 - mae: 13.8672 - val_loss: 922.3921 - val_mae: 20.2496\n",
      "Epoch 486/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 360.9413 - mae: 13.8276 - val_loss: 929.3029 - val_mae: 20.2335\n",
      "Epoch 487/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 361.6444 - mae: 13.8331 - val_loss: 922.9016 - val_mae: 20.2105\n",
      "Epoch 488/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 362.3145 - mae: 13.8698 - val_loss: 929.1299 - val_mae: 20.3989\n",
      "Epoch 489/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 360.8961 - mae: 13.8165 - val_loss: 918.4844 - val_mae: 20.1885\n",
      "Epoch 490/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 361.5021 - mae: 13.8460 - val_loss: 918.0445 - val_mae: 20.2403\n",
      "Epoch 491/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 361.5131 - mae: 13.8576 - val_loss: 925.0427 - val_mae: 20.2333\n",
      "Epoch 492/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 363.0078 - mae: 13.8506 - val_loss: 927.6497 - val_mae: 20.3757\n",
      "Epoch 493/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 361.3330 - mae: 13.8496 - val_loss: 925.2402 - val_mae: 20.2066\n",
      "Epoch 494/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 362.4926 - mae: 13.8555 - val_loss: 925.7610 - val_mae: 20.1764\n",
      "Epoch 495/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 361.0697 - mae: 13.8249 - val_loss: 917.0070 - val_mae: 20.1803\n",
      "Epoch 496/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 360.3454 - mae: 13.8175 - val_loss: 928.6487 - val_mae: 20.2226\n",
      "Epoch 497/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 361.8700 - mae: 13.8259 - val_loss: 925.4319 - val_mae: 20.2105\n",
      "Epoch 498/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 361.1818 - mae: 13.8334 - val_loss: 927.8823 - val_mae: 20.3124\n",
      "Epoch 499/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 359.2590 - mae: 13.8028 - val_loss: 925.4641 - val_mae: 20.2638\n",
      "Epoch 500/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 361.2901 - mae: 13.8009 - val_loss: 920.2542 - val_mae: 20.2466\n",
      "Epoch 501/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 358.3858 - mae: 13.7992 - val_loss: 920.2432 - val_mae: 20.1879\n",
      "Epoch 502/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 357.6155 - mae: 13.7495 - val_loss: 922.9285 - val_mae: 20.2449\n",
      "Epoch 503/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 359.4861 - mae: 13.7952 - val_loss: 927.6472 - val_mae: 20.3367\n",
      "Epoch 504/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 357.6528 - mae: 13.7671 - val_loss: 923.9117 - val_mae: 20.1994\n",
      "Epoch 505/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 359.9753 - mae: 13.7961 - val_loss: 928.2840 - val_mae: 20.2908\n",
      "Epoch 506/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 355.9602 - mae: 13.7229 - val_loss: 918.3213 - val_mae: 20.1295\n",
      "Epoch 507/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 359.3345 - mae: 13.7888 - val_loss: 928.1782 - val_mae: 20.2797\n",
      "Epoch 508/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 357.9144 - mae: 13.7891 - val_loss: 924.8942 - val_mae: 20.2412\n",
      "Epoch 509/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 357.6575 - mae: 13.7731 - val_loss: 921.1946 - val_mae: 20.1561\n",
      "Epoch 510/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 358.2109 - mae: 13.7783 - val_loss: 925.5490 - val_mae: 20.3464\n",
      "Epoch 511/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 357.2662 - mae: 13.7701 - val_loss: 929.5454 - val_mae: 20.2837\n",
      "Epoch 512/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 358.1786 - mae: 13.7883 - val_loss: 919.3118 - val_mae: 20.1900\n",
      "Epoch 513/1000\n",
      "438/438 [==============================] - 14s 32ms/step - loss: 356.0335 - mae: 13.7495 - val_loss: 929.4432 - val_mae: 20.2490\n",
      "Epoch 514/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 356.8154 - mae: 13.7795 - val_loss: 930.8029 - val_mae: 20.3794\n",
      "Epoch 515/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 355.0119 - mae: 13.7226 - val_loss: 924.1458 - val_mae: 20.2717\n",
      "Epoch 516/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 354.4322 - mae: 13.7180 - val_loss: 918.9508 - val_mae: 20.2011\n",
      "Epoch 517/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 356.8756 - mae: 13.7410 - val_loss: 915.2114 - val_mae: 20.1632\n",
      "Epoch 518/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 359.3561 - mae: 13.7640 - val_loss: 926.0753 - val_mae: 20.2301\n",
      "Epoch 519/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 355.7325 - mae: 13.7211 - val_loss: 925.1914 - val_mae: 20.3138\n",
      "Epoch 520/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 352.7652 - mae: 13.6946 - val_loss: 921.2717 - val_mae: 20.1824\n",
      "Epoch 521/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 353.7542 - mae: 13.6903 - val_loss: 923.0103 - val_mae: 20.2287\n",
      "Epoch 522/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 358.7825 - mae: 13.7683 - val_loss: 915.4029 - val_mae: 20.1325\n",
      "Epoch 523/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 356.4437 - mae: 13.7374 - val_loss: 918.5663 - val_mae: 20.1794\n",
      "Epoch 524/1000\n",
      "438/438 [==============================] - 15s 34ms/step - loss: 354.5529 - mae: 13.7270 - val_loss: 925.4518 - val_mae: 20.3193\n",
      "Epoch 525/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 354.5115 - mae: 13.7096 - val_loss: 918.6874 - val_mae: 20.2056\n",
      "Epoch 526/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 352.5606 - mae: 13.6674 - val_loss: 916.9379 - val_mae: 20.1003\n",
      "Epoch 527/1000\n",
      "438/438 [==============================] - 14s 33ms/step - loss: 352.3927 - mae: 13.6697 - val_loss: 919.8402 - val_mae: 20.1696\n",
      "Epoch 528/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 352.1483 - mae: 13.6720 - val_loss: 921.5206 - val_mae: 20.2217\n",
      "Epoch 529/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 353.2665 - mae: 13.6791 - val_loss: 917.9814 - val_mae: 20.1349\n",
      "Epoch 530/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 353.6027 - mae: 13.6868 - val_loss: 914.8455 - val_mae: 20.1567\n",
      "Epoch 531/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 352.9193 - mae: 13.7045 - val_loss: 918.3065 - val_mae: 20.1793\n",
      "Epoch 532/1000\n",
      "438/438 [==============================] - 13s 30ms/step - loss: 352.0976 - mae: 13.6657 - val_loss: 922.5311 - val_mae: 20.2851\n",
      "Epoch 533/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 350.8550 - mae: 13.6462 - val_loss: 923.5452 - val_mae: 20.1902\n",
      "Epoch 534/1000\n",
      "438/438 [==============================] - 13s 31ms/step - loss: 351.3323 - mae: 13.6381 - val_loss: 925.7148 - val_mae: 20.2420\n",
      "Epoch 535/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 353.7263 - mae: 13.6982 - val_loss: 918.3533 - val_mae: 20.1010\n",
      "Epoch 536/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 350.6603 - mae: 13.6320 - val_loss: 914.8861 - val_mae: 20.0618\n",
      "Epoch 537/1000\n",
      "438/438 [==============================] - 14s 31ms/step - loss: 354.0444 - mae: 13.6835 - val_loss: 915.2346 - val_mae: 20.1772\n",
      "Epoch 538/1000\n",
      "438/438 [==============================] - 10s 22ms/step - loss: 356.0936 - mae: 13.7241 - val_loss: 920.2722 - val_mae: 20.2250\n",
      "Epoch 539/1000\n",
      "407/438 [==========================>...] - ETA: 0s - loss: 350.8945 - mae: 13.6425"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 98\u001b[0m\n\u001b[0;32m     93\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     94\u001b[0m               loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     95\u001b[0m               metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     97\u001b[0m \u001b[39m# 모델 학습\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(x_val, y_val))\n\u001b[0;32m    100\u001b[0m \u001b[39m# 예측\u001b[39;00m\n\u001b[0;32m    101\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(x_test)\n",
      "File \u001b[1;32mc:\\Users\\isshe\\anaconda3\\envs\\sf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\isshe\\anaconda3\\envs\\sf\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\isshe\\anaconda3\\envs\\sf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\isshe\\anaconda3\\envs\\sf\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\isshe\\anaconda3\\envs\\sf\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\isshe\\anaconda3\\envs\\sf\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\isshe\\anaconda3\\envs\\sf\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\isshe\\anaconda3\\envs\\sf\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\isshe\\anaconda3\\envs\\sf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.metrics import MeanSquaredError\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 이미지 데이터셋과 CSV 파일 경로\n",
    "image_folder = \"./sf/\"  # 이미지 폴더 경로\n",
    "csv_file = \"./joint_all.csv\"  # CSV 파일 경로\n",
    "\n",
    "# 이미지 크기\n",
    "image_size = (32, 32)\n",
    "\n",
    "# 이미지 데이터(특징값) 불러오는 함수\n",
    "def load_images(image_folder, image_size):\n",
    "    images = []\n",
    "    for filename in os.listdir(image_folder):\n",
    "        img = cv2.imread(os.path.join(image_folder, filename), cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            _, binary_image = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "            binary_image = binary_image.astype(np.uint8)\n",
    "            distance_transform = cv2.distanceTransform(binary_image, cv2.DIST_L2, 5)\n",
    "            img = cv2.resize(distance_transform, image_size)\n",
    "            img = img.flatten()\n",
    "            images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "# CSV 파일 불러오는 함수\n",
    "def load_csv(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    return df.values[:, 1:]  # 첫 번째 열(이미지 이름) 제외\n",
    "\n",
    "# 이미지 데이터셋과 CSV 파일 로드\n",
    "image_data = load_images(image_folder, image_size)\n",
    "csv_data = load_csv(csv_file)\n",
    "\n",
    "for i in range(len(csv_data)):\n",
    "    if csv_data[i][6] > csv_data[i][10]:\n",
    "        for j in range(6,10):\n",
    "            tmp = csv_data[i][j]\n",
    "            csv_data[i][j] = csv_data[i][j+4]\n",
    "            csv_data[i][j+4] = tmp\n",
    "\n",
    "    if csv_data[i][14] > csv_data[i][18]:\n",
    "        for j in range(14,18):\n",
    "            tmp = csv_data[i][j]\n",
    "            csv_data[i][j] = csv_data[i][j+4]\n",
    "            csv_data[i][j+4] = tmp\n",
    "\n",
    "# 데이터셋 셔플링\n",
    "indices = np.arange(len(csv_data))\n",
    "np.random.shuffle(indices)\n",
    "shuffled_images = image_data[indices]\n",
    "shuffled_csv = csv_data[indices]\n",
    "\n",
    "\n",
    "# 데이터셋 분할 (train:validation:test = 0.6:0.2:0.2)\n",
    "train_images, test_images, train_csv, test_csv = train_test_split(shuffled_images, shuffled_csv, test_size=0.2, random_state=42)\n",
    "train_images, val_images, train_csv, val_csv = train_test_split(train_images, train_csv, test_size=0.25, random_state=42)\n",
    "\n",
    "# 데이터 전처리 (입력 이미지와 목표값 간의 매핑 필요)\n",
    "x_train = train_images.reshape(-1, *image_size, 1).astype('float32') / 255.0\n",
    "y_train = train_csv.astype('float32')\n",
    "\n",
    "x_val = val_images.reshape(-1, *image_size, 1).astype('float32') / 255.0\n",
    "y_val = val_csv.astype('float32')\n",
    "\n",
    "x_test = test_images.reshape(-1, *image_size, 1).astype('float32') / 255.0\n",
    "y_test = test_csv.astype('float32')\n",
    "\n",
    "# 모델 구축\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(*image_size, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(22))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mae'])\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(x_train, y_train, epochs=1000, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "# 예측\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# R^2 점수 계산\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R^2 Score:\", r2)\n",
    "\n",
    "# 손실과 정확도 그래프 그리기\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Mean Absolute Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eecd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 242ms/step\n",
      "x_ex 예측 좌표값: [[181.47876  180.70934  164.04144   99.995285 128.10104   79.019905\n",
      "  125.06955  109.81889   90.620476 109.66133  168.92056  115.19126\n",
      "  166.94041  153.8494   118.97757  208.30713  116.84224  276.5387\n",
      "  211.5402   208.48203  229.2586   259.81815 ]]\n",
      "실제 x_ex 좌표값: []\n",
      "오차: []\n",
      "오차 평균의 절대값: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\isshe\\anaconda3\\envs\\sf\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\isshe\\anaconda3\\envs\\sf\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ex_image = load_images(\"./image_ex/\", image_size)\n",
    "ex_csv = load_csv(\"./joint_ex.csv\")\n",
    "\n",
    "x_ex = ex_image.reshape(-1, *image_size, 1).astype('float32') / 255.0\n",
    "y_ex = ex_csv.astype('float32')\n",
    "\n",
    "predictions = model.predict(x_ex)\n",
    "\n",
    "print(\"x_ex 예측 좌표값:\", predictions)  \n",
    "print(\"실제 x_ex 좌표값:\", ex_csv)\n",
    "print(\"오차:\", predictions - ex_csv)\n",
    "\n",
    "# 오차 평균 구하기\n",
    "average = np.mean(abs(predictions - y_ex))\n",
    "print(\"오차 평균의 절대값:\", abs(average))\n",
    "\n",
    "imageFile = './image_ex/sf_t.png'\n",
    "X_ex_image = cv2.imread(imageFile)\n",
    "\n",
    "# 각 관절 위치 값 저장\n",
    "data=predictions[0]\n",
    "\n",
    "c1_x = int(round(data[0]))           \n",
    "c1_y = int(round(data[1]))            \n",
    "c2_x = int(round(data[2]))             \n",
    "c2_y = int(round(data[3]))             \n",
    "c3_x = int(round(data[4]))             \n",
    "c3_y = int(round(data[5]))             \n",
    "le_x = int(round(data[6]))             \n",
    "le_y = int(round(data[7]))            \n",
    "lh_x = int(round(data[8]))             \n",
    "lh_y = int(round(data[9]))             \n",
    "re_x = int(round(data[10]))             \n",
    "re_y = int(round(data[11]))             \n",
    "rh_x = int(round(data[12]))             \n",
    "rh_y = int(round(data[13]))             \n",
    "lk_x = int(round(data[14]))             \n",
    "lk_y = int(round(data[15]))              \n",
    "lf_x = int(round(data[16]))             \n",
    "lf_y = int(round(data[17]))             \n",
    "rk_x = int(round(data[18]))             \n",
    "rk_y = int(round(data[19]))             \n",
    "rf_x = int(round(data[20]))             \n",
    "rf_y = int(round(data[21]))  \n",
    "\n",
    "\n",
    "\n",
    "cv2.circle(X_ex_image,(c1_x,c1_y), radius=5, color=(0,0,255), thickness=-1)  #빨\n",
    "cv2.circle(X_ex_image,(c2_x,c2_y), radius=5, color=(0,94,255), thickness=-1)  #주\n",
    "cv2.circle(X_ex_image,(c3_x,c3_y), radius=5, color=(0,228,255), thickness=-1) #노\n",
    "cv2.circle(X_ex_image,(le_x,le_y), radius=5, color=(0,255,0), thickness=-1)  #초\n",
    "cv2.circle(X_ex_image,(lh_x,lh_y), radius=5, color=(255,0,0), thickness=-1)  #파\n",
    "cv2.circle(X_ex_image,(re_x,re_y), radius=5, color=(255,0,95), thickness=-1)\n",
    "cv2.circle(X_ex_image,(rh_x,rh_y), radius=5, color=(128,0,128), thickness=-1)\n",
    "cv2.circle(X_ex_image,(lk_x,lk_y), radius=5, color=(255,0,255), thickness=-1)\n",
    "cv2.circle(X_ex_image,(lf_x,lf_y), radius=5, color=(128,128,128), thickness=-1)\n",
    "cv2.circle(X_ex_image,(rk_x,rk_y), radius=5, color=(0,128,128), thickness=-1)\n",
    "cv2.circle(X_ex_image,(rf_x,rf_y), radius=5, color=(128,128,0), thickness=-1)\n",
    "\n",
    "\n",
    "img = cv2.cvtColor(X_ex_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#이미지, 점 출력\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
